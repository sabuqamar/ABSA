{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from -r ../requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: selenium in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from -r ../requirements.txt (line 2)) (4.9.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from -r ../requirements.txt (line 3)) (4.28.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from -r ../requirements.txt (line 4)) (1.2.2)\n",
      "Requirement already satisfied: torch in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from -r ../requirements.txt (line 5)) (2.0.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from -r ../requirements.txt (line 6)) (4.12.2)\n",
      "Collecting argparse\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: datasets in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from -r ../requirements.txt (line 8)) (2.12.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from -r ../requirements.txt (line 9)) (1.24.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from -r ../requirements.txt (line 10)) (4.65.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from -r ../requirements.txt (line 11)) (3.8.1)\n",
      "Requirement already satisfied: tokenizer in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from -r ../requirements.txt (line 12)) (3.4.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from pandas->-r ../requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from pandas->-r ../requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from pandas->-r ../requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from selenium->-r ../requirements.txt (line 2)) (0.22.0)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from selenium->-r ../requirements.txt (line 2)) (1.26.15)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from selenium->-r ../requirements.txt (line 2)) (0.10.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from selenium->-r ../requirements.txt (line 2)) (2022.12.7)\n",
      "Requirement already satisfied: filelock in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from transformers->-r ../requirements.txt (line 3)) (3.12.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from transformers->-r ../requirements.txt (line 3)) (2023.5.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from transformers->-r ../requirements.txt (line 3)) (0.14.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from transformers->-r ../requirements.txt (line 3)) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from transformers->-r ../requirements.txt (line 3)) (23.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from transformers->-r ../requirements.txt (line 3)) (0.13.3)\n",
      "Requirement already satisfied: requests in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from transformers->-r ../requirements.txt (line 3)) (2.30.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from scikit-learn->-r ../requirements.txt (line 4)) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from scikit-learn->-r ../requirements.txt (line 4)) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from scikit-learn->-r ../requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from torch->-r ../requirements.txt (line 5)) (4.5.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from torch->-r ../requirements.txt (line 5)) (3.1.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from torch->-r ../requirements.txt (line 5)) (3.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from torch->-r ../requirements.txt (line 5)) (1.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from beautifulsoup4->-r ../requirements.txt (line 6)) (2.4.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from datasets->-r ../requirements.txt (line 8)) (3.2.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from datasets->-r ../requirements.txt (line 8)) (0.3.6)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from datasets->-r ../requirements.txt (line 8)) (0.70.14)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from datasets->-r ../requirements.txt (line 8)) (12.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from datasets->-r ../requirements.txt (line 8)) (2023.4.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from datasets->-r ../requirements.txt (line 8)) (3.8.4)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from datasets->-r ../requirements.txt (line 8)) (0.18.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from tqdm->-r ../requirements.txt (line 10)) (0.4.6)\n",
      "Requirement already satisfied: click in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from nltk->-r ../requirements.txt (line 11)) (8.1.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from aiohttp->datasets->-r ../requirements.txt (line 8)) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from aiohttp->datasets->-r ../requirements.txt (line 8)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from aiohttp->datasets->-r ../requirements.txt (line 8)) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from aiohttp->datasets->-r ../requirements.txt (line 8)) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from aiohttp->datasets->-r ../requirements.txt (line 8)) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from aiohttp->datasets->-r ../requirements.txt (line 8)) (3.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from aiohttp->datasets->-r ../requirements.txt (line 8)) (1.9.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r ../requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from requests->transformers->-r ../requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from trio~=0.17->selenium->-r ../requirements.txt (line 2)) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from trio~=0.17->selenium->-r ../requirements.txt (line 2)) (1.15.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from trio~=0.17->selenium->-r ../requirements.txt (line 2)) (1.1.1)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from trio~=0.17->selenium->-r ../requirements.txt (line 2)) (1.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from trio~=0.17->selenium->-r ../requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from trio~=0.17->selenium->-r ../requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from trio-websocket~=0.9->selenium->-r ../requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from urllib3[socks]~=1.26->selenium->-r ../requirements.txt (line 2)) (1.7.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from jinja2->torch->-r ../requirements.txt (line 5)) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from sympy->torch->-r ../requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium->-r ../requirements.txt (line 2)) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\sahil\\documents\\nlu\\absa\\venv\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->-r ../requirements.txt (line 2)) (0.14.0)\n",
      "Installing collected packages: argparse\n",
      "Successfully installed argparse-1.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import csv\n",
    "\n",
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "# download the necessary nltk packages (only needs to be done once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def read_reviews_from_csv(file_path):\n",
    "    ratings = []\n",
    "    reviews = []\n",
    "    with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            # Assuming rating is the first item in each row, and review is the second item\n",
    "            rating, review = row[0], row[1]\n",
    "            ratings.append(int(rating)) # Convert rating to integer if needed\n",
    "            reviews.append(review)\n",
    "    return ratings, reviews\n",
    "\n",
    "\n",
    "# function to clean a review text\n",
    "def clean_review(review):\n",
    "    # convert to lowercase\n",
    "    review = review.lower()\n",
    "    # remove non-alphanumeric characters\n",
    "    review = re.sub(r'[^a-zA-Z0-9\\s]', '', review)\n",
    "    # remove extra whitespaces\n",
    "    review = re.sub(r'\\s+', ' ', review).strip()\n",
    "    # tokenize the cleaned review into words using nltk's word_tokenize() function\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    tokens = [str(token) for token in tokens]\n",
    "    # join the tokens back into a sentence using ' ' as a separator\n",
    "    review = ' '.join(tokens)\n",
    "    # return cleaned review\n",
    "    return review\n",
    "\n",
    "\n",
    "def get_ratings_sentences(path):\n",
    "    ratings, reviews = read_reviews_from_csv(path)\n",
    "    # clean the reviews and split them into sentences\n",
    "    sentences = []\n",
    "    for idx, review in enumerate(reviews):\n",
    "        # clean the review text\n",
    "        cleaned_review = clean_review(review)\n",
    "        # split the cleaned review into sentences using nltk's sent_tokenize() function\n",
    "        review_sentences = nltk.sent_tokenize(cleaned_review)\n",
    "        # append the sentences to the list\n",
    "        sentences.extend(review_sentences)\n",
    "\n",
    "    return sentences\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call Model \n",
    "Instruct ATE + SA + Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sahil\\Documents\\NLU\\ABSA\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: like the restaurant the burgers are delicious but for the price you pay they are cheap with the fries like the decorations and music the store is a plus our waitress very friendly\n",
      "Aspects: ['burgers', 'price', 'fries', 'decorations', 'music', 'waitress']\n",
      "Final Aspects: ['burgers', 'price', 'fries', 'decorations', 'music', 'waitress']\n",
      "Polarities:  [0, 0, 2, 0, 1, 0]\n",
      "Clustering scores:  [1, 0, 1, 1]\n",
      "Sentence: coming from the uk and knowing how popular this chain is i prebooked our table well in advance of our visit to nyc last time we came to the city we were unable to get a walkin that suited so didnt want to face the same disfortune this time round we attended on a wednesday night in early december and after a small que awaiting a photo being taken we got seated admittedly our table didnt feel in the best location within the restaurant but despite this we had no issues with it our server was friendly and welcoming orders were taken swiftly and drinks and food followed accordingly i had a burger followed by an apple cobbler dessert and washed down with a hurricane cocktail all were delicious and filled the gap we opted to pay extra to keep our cocktail glasses and were given receipts to collect fresh ones from the till upstairs on our departure there we were given options as to whether we wanted standard or commemorative glasses i love the hard rock chain and love to visit those i get the opportunity to across the world new york didnt disappoint and i would return in the future\n",
      "Aspects: ['server', 'drinks', 'food', 'burger', 'apple cobbler dessert', 'served', 'waitress', 'glasses']\n",
      "Final Aspects: ['server', 'drinks', 'food', 'burger', 'apple', 'glasses']\n",
      "Polarities:  [0, 1, 1, 2, 1, 2]\n",
      "Clustering scores:  [0, -2, 1, 0]\n",
      "Sentence: i ordered the original burger with fries to takeout the entire experience from the time i walked in was not good the person at the front directed me to the bar to take my order which was far away from the entrance at the bar no one wanted to or was available to take my order when i approached two different staff members it seemed like the excuse was it wasnt their job and to go to the next person when i finally found someone willing to take my order they told me it was going to be a 20 minute wait i waited patiently after 25 mins there was no update and so i approached the person who took my order it turns out he forgot about it and he went to the kitchen to get it as they dont take out your order from the kitchen unless a staff member goes out the back to retrieve it i walked back to my hotel which was only 5 mins away i opened my bag to find soggy cold chips and even worse a burger which had all the fillings but no sauce it was a totally dry burger this was an awful experience unattentive staff who couldnt care less and forgot about my order making me wait more than 25 mins for a simple and absolutely atrocious burger and the whole meal was equally atrocious because it was cold soggy with a sauceless burger needless to say it was a terrible meal ill never go back again and i do not recommend the entire hard rock cafe chain anywhere in the world i went to the one in florence in april and although it was ok the fries were no good either and the burger was average at best but at least it had sauce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (620 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspects: ['burger with fries', 'burger', 'meal']\n",
      "Final Aspects: ['burger', 'burger', 'meal']\n",
      "Polarities:  [2, 2, 2]\n",
      "Clustering scores:  [0, -2, -1, 0]\n",
      "Sentence: was in nyc 10 days ago and during my short 5 days stay went twice the day i arrived and the day i was leaving totally worth it not long wait good service great food good pricequality relation totally nice decor as all hr cafes this one has an strategic location right at times square in the middle of the best in my opinion manhattan touristic spot i need to make a special mention to the server we got on our lunch on 24th at about 1145am dont know her name but we were 2 sitting from the door to the right not far from the tall tables like bar ones we got a messi burger the spicy shrimps and a salmon and noodle bowl and the server we got was just amazing i am so sorry did not catch her name i really hope you could find out because she was extraordinary kind nice super smiley super knowledgeable of everything attentive fast empathic everything a great server should have she had it always coming back to hrc every time i am in ncy see you next time thanks for keeping the great job\n",
      "Aspects: ['service', 'food', 'pricequality relation', 'server', 'lunch']\n",
      "Final Aspects: ['service', 'food', 'pricequality', 'server', 'lunch']\n",
      "Polarities:  [0, 0, 0, 2, 1]\n",
      "Clustering scores:  [1, 1, 0, 0]\n",
      "Sentence: could not fault the service or food it was so nice we just didnt want it to end my only criticism was the prices on the menu did not match the receipt for example a bud light was 750 but were then charged 850 each also not happy having to pay 1299 for the first pint so i could have a glass that i didnt really want\n",
      "Aspects: ['service', 'food', 'prices', 'menu', 'receipt', 'bud light', 'pint', 'glass']\n",
      "Final Aspects: ['service', 'food', 'prices', 'menu', 'receipt', 'bud', 'pint', 'glass']\n",
      "Polarities:  [0, 0, 2, 2, 2, 2, 2, 2]\n",
      "Clustering scores:  [1, -2, -3, 0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Add the parent directory of ABSA to the module search path\n",
    "sys.path.append('..')\n",
    "\n",
    "from models.SA import evaluate\n",
    "from models.Clustering import scores\n",
    "from scripts import script\n",
    "sentences = get_ratings_sentences(\"../data/reviews.csv\")\n",
    "category_1_score = 0\n",
    "category_1_total = 0\n",
    "category_2_score = 0\n",
    "category_2_total = 0\n",
    "category_3_score = 0\n",
    "category_3_total = 0\n",
    "category_4_score = 0\n",
    "category_4_total = 0\n",
    "\n",
    "for sent in sentences:\n",
    "\tprint(\"Sentence:\" , sent)\n",
    "\tterms = script.return_iabsa(script.Task.ATE, \"../models/InstructABSA/InstructABSA/Models/ate/allenaitk-instruct-base-def-pos-ate_check\", 2, sent)\n",
    "\tprint(\"Aspects:\" , terms)\n",
    "\tlabels, final_terms = evaluate.evaluate(sent, terms)\n",
    "\tprint(\"Final Aspects:\" , final_terms)\n",
    "\tprint(\"Polarities: \" , labels)\n",
    "\tcategories_scores, categories_total =  scores.get_clusters(final_terms, labels)\n",
    "\tprint(\"Clustering scores: \", categories_scores)\n",
    "\tcategory_1_score += categories_scores[0]\n",
    "\tcategory_1_total += categories_total[0]\n",
    "\tcategory_2_score += categories_scores[1]\n",
    "\tcategory_2_total += categories_total[1]\n",
    "\tcategory_3_score += categories_scores[2]\n",
    "\tcategory_3_total += categories_total[2]\n",
    "\tcategory_4_score += categories_scores[3]\n",
    "\tcategory_4_total += categories_total[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurant 1 Sentiment Score\n",
      "-----------------\n",
      "service:  3\n",
      "food:  -5\n",
      "price:  -2\n",
      "ambience:  1\n"
     ]
    }
   ],
   "source": [
    "rest_1_scores = {'service': category_1_score, 'food': category_2_score, 'price': category_3_score, 'ambience': category_4_score}\n",
    "print(\"Restaurant 1 Sentiment Score\")\n",
    "print(\"-----------------\")\n",
    "print(\"service: \", category_1_score)\n",
    "print(\"food: \", category_2_score)\n",
    "print(\"price: \", category_3_score)\n",
    "print(\"ambience: \", category_4_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: this was our first visit to the rake lovely premises music playing and we were welcomed in we were shown to our table and offered drinks the menu arrived a strip of paper and pen and we made our selections it would have been helpful to have a detailed menu so we could understand the dishes ingredients etc the salt and pepper on our table was from the coop the food was good however our dishes were brought out one at a time which tended to result in us not eating at the same time 110 bill for two people seemed pricey in our view\n",
      "Aspects: ['premises', 'food', 'dishes', 'salt and pepper on our table', 'drinks', 'menu', 'dishes', 'bill']\n",
      "Final Aspects: ['premises', 'food', 'dishes', 'salt', 'drinks', 'menu', 'dishes', 'bill']\n",
      "Polarities:  [0, 1, 2, 2, 1, 2, 2, 2]\n",
      "Clustering scores:  [0, -4, 0, 0]\n",
      "Sentence: another fabulous meal at the rake the staff were wonderful especially our italian waitress really enjoyed the food will be back very soon\n",
      "Aspects: ['staff', 'italian waitress', 'food']\n",
      "Final Aspects: ['staff', 'italian', 'food']\n",
      "Polarities:  [0, 0, 0]\n",
      "Clustering scores:  [1, 2, 0, 0]\n",
      "Sentence: fantastic food and fantastic service it was our first visit here and will be back again well worth a visit\n",
      "Aspects: ['food', 'service']\n",
      "Final Aspects: ['food', 'service']\n",
      "Polarities:  [0, 0]\n",
      "Clustering scores:  [1, 1, 0, 0]\n",
      "Sentence: food amazing staff so polite and professional lovely atmosphere worth the taxi from rochdale will visit again\n",
      "Aspects: ['food', 'staff', 'atmosphere']\n",
      "Final Aspects: ['food', 'staff', 'atmosphere']\n",
      "Polarities:  [0, 0, 0]\n",
      "Clustering scores:  [1, 2, 0, 0]\n",
      "Sentence: delicious foodcosy atmospherewide selection of tapas and great service will definitely return and would recommend a visit\n",
      "Aspects: ['food', 'atmosphere', 'tapas', 'service']\n",
      "Final Aspects: ['tapas', 'service']\n",
      "Polarities:  [0, 0]\n",
      "Clustering scores:  [1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "sentences = get_ratings_sentences(\"../data/reviews2.csv\")\n",
    "category_1_score_2 = 0\n",
    "category_2_score_2 = 0\n",
    "category_3_score_2 = 0\n",
    "category_4_score_2 = 0\n",
    "\n",
    "for sent in sentences:\n",
    "\tprint(\"Sentence:\" , sent)\n",
    "\tterms = script.return_iabsa(script.Task.ATE, \"../models/InstructABSA/InstructABSA/Models/ate/allenaitk-instruct-base-def-pos-ate_check\", 2, sent)\n",
    "\tprint(\"Aspects:\" , terms)\n",
    "\tlabels, final_terms = evaluate.evaluate(sent, terms)\n",
    "\tprint(\"Final Aspects:\" , final_terms)\n",
    "\tprint(\"Polarities: \" , labels)\n",
    "\tcategories_scores, categories_total =  scores.get_clusters(final_terms, labels)\n",
    "\tprint(\"Clustering scores: \", categories_scores)\n",
    "\tcategory_1_score_2 += categories_scores[0]\n",
    "\tcategory_2_score_2 += categories_scores[1]\n",
    "\tcategory_3_score_2 += categories_scores[2]\n",
    "\tcategory_4_score_2 += categories_scores[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurant 2 Sentiment Scores\n",
      "-----------------\n",
      "service:  4\n",
      "food:  2\n",
      "price:  0\n",
      "ambience:  0\n"
     ]
    }
   ],
   "source": [
    "rest_2_scores = {'service': category_1_score_2, 'food': category_2_score_2, 'price': category_3_score_2, 'ambience': category_4_score_2}\n",
    "\n",
    "print(\"Restaurant 2 Sentiment Scores\")\n",
    "print(\"-----------------\")\n",
    "print(\"service: \", category_1_score_2)\n",
    "print(\"food: \", category_2_score_2)\n",
    "print(\"price: \", category_3_score_2)\n",
    "print(\"ambience: \", category_4_score_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurant 1 or 2?\n",
      "-----------------\n",
      "service: Restaurant 2 has better service than Restaurant 1.\n",
      "food: Restaurant 2 has better food than Restaurant 1.\n",
      "price: Restaurant 2 has better price than Restaurant 1.\n",
      "ambience: Restaurant 1 has better ambience than Restaurant 2.\n"
     ]
    }
   ],
   "source": [
    "print(\"Restaurant 1 or 2?\")\n",
    "print(\"-----------------\")\n",
    "for attribute in rest_1_scores:\n",
    "    if rest_1_scores[attribute] > rest_2_scores[attribute]:\n",
    "        print(f\"{attribute}: Restaurant 1 has better {attribute} than Restaurant 2.\")\n",
    "    elif rest_1_scores[attribute] < rest_2_scores[attribute]:\n",
    "        print(f\"{attribute}: Restaurant 2 has better {attribute} than Restaurant 1.\")\n",
    "    else:\n",
    "        print(f\"{attribute}: Restaurant 1 and Restaurant 2 have the same level of {attribute}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
