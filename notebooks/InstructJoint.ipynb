{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizer==3.4.2 (from -r ../requirements.txt (line 1))\n",
      "  Using cached tokenizer-3.4.2-py2.py3-none-any.whl (79 kB)\n",
      "Installing collected packages: tokenizer\n",
      "Successfully installed tokenizer-3.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# download the necessary nltk packages (only needs to be done once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def read_reviews_from_csv(file_path):\n",
    "    ratings = []\n",
    "    reviews = []\n",
    "    with open(file_path, 'r', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            # Assuming rating is the first item in each row, and review is the second item\n",
    "            rating, review = row[0], row[1]\n",
    "            ratings.append(int(rating)) # Convert rating to integer if needed\n",
    "            reviews.append(review)\n",
    "    return ratings, reviews\n",
    "\n",
    "\n",
    "# function to clean a review text\n",
    "def clean_review(review):\n",
    "    # convert to lowercase\n",
    "    review = review.lower()\n",
    "    # remove non-alphanumeric characters\n",
    "    review = re.sub(r'[^a-zA-Z0-9\\s]', '', review)\n",
    "    # remove extra whitespaces\n",
    "    review = re.sub(r'\\s+', ' ', review).strip()\n",
    "    # return cleaned review\n",
    "    return review\n",
    "\n",
    "\n",
    "def get_ratings_sentences():\n",
    "\tratings, reviews = read_reviews_from_csv(\"../data/reviews.csv\")\n",
    "\t# clean the reviews and split them into sentences\n",
    "\tsentences = []\n",
    "\tnew_ratings = []\n",
    "\tfor idx, review in enumerate(reviews):\n",
    "\t\t# clean the review text\n",
    "\t\tcleaned_review = clean_review(review)\n",
    "\t\t# split the cleaned review into sentences\n",
    "\t\treview_sentences = nltk.sent_tokenize(cleaned_review)\n",
    "\t\t# append the sentences to the list\n",
    "\t\tsentences.extend(review_sentences)\n",
    "\t\tnew_ratings.extend(ratings[idx])\n",
    "\n",
    "\treturn sentences, new_ratings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call Model\n",
    "Instruct InstructJoint + Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Add the parent directory of ABSA to the module search path\n",
    "sys.path.append('..')\n",
    "from models.Clustering import scores\n",
    "from scripts import script\n",
    "sentences, new_ratings = get_ratings_sentences()\n",
    "for sent in sentences:\n",
    "    # call ate\n",
    "\tterms, labels = script.return_iabsa(script.Task.JOINT, \"./Models/joint/allenaitk_instruct_base_def_pos_jointtask_check\", 2, \"The cab ride was amazing but the service was pricey\")\n",
    "\tscores.get_clusters(terms, labels)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
